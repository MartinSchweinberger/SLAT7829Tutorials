{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](https://github.com/MartinSchweinberger/SLAT7829/blob/master/images/bannerSLAT7829.jpeg?raw=true)\n",
                "\n",
                "# Analyzing n-grams, collocations, and keyness\n",
                "\n",
                "This tutorial show how you can extract and analyze n-grams, collocations, and keyness.\n",
                "\n",
                "However, please keep in mind that the case studies  merely aim to exemplify ways in which R can be used in language-based research - rather than providing detailed procedures on how to do corpus-based research. \n",
                "\n",
                "\n",
                "Collocation refers to the co-occurrence of words. A typical example of a collocation is *Merry Christmas* because the words *merry* and *Christmas* occur together more frequently together than would be expected by chance, if words were just randomly stringed together.\n",
                "\n",
                "N-grams are related to collocates in that they represent words that occur together (bi-grams are two words that occur together, tri-grams three words and so on). Fortunately, creating N-gram lists is very easy. We will use the example text to create a bi-gram list. We can simply take each word and combine it with the following word.\n",
                "\n",
                "## Preparation and session set up\n",
                "\n",
                "Activate required packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(dplyr)\n",
                "library(tidyr)\n",
                "library(stringr)\n",
                "library(ggplot2)\n",
                "library(quanteda)\n",
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading data\n",
                "\n",
                "To analyze n-grams and collocations, we first need to load some data. In this tutorial, we will use the Australian Corpus of English (ACE) and the BROWN corpus which represents American English data.\n",
                "\n",
                "Loading corpus data into R consists of two steps: \n",
                "\n",
                "1. create a list of paths of the corpus files\n",
                "\n",
                "2. loop over these paths and load the data in the files identified by the paths.\n",
                "\n",
                "To create a list of corpus files, you could use the code chunk below (the code chunk assumes that the BROWN data is in a folder called *BROWN* and the LOB data is in a folder called *LOB*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load ace files\n",
                "acefiles <- list.files(here::here(\"ACE\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load the files by scanning the content\n",
                "ace <- sapply(acefiles, function(x){\n",
                "  x <- scan(x, what = \"char\",  sep = \"\", quote = \"\",  quiet = T,  skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(ace)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we load the BROWN data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load ace files\n",
                "brownfiles <- list.files(here::here(\"BROWN\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load the files by scanning the content\n",
                "brown <- sapply(brownfiles, function(x){\n",
                "  x <- scan(x, what = \"char\",  sep = \"\", quote = \"\",  quiet = T,  skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(ace); str(brown)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "You can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "Then, click on the `New Folder` symbol and create a new folder and call it `MyData`.\n",
                "\n",
                "![Binder New Folder Symbol](https://slcladal.github.io/images/bindernewfolder.JPG)\n",
                "\n",
                "Then click on the upload symbol and upload your files into the `MyData` folder.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT**: here, we assume that you upload some form of text data - not tabular data!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mycorpus <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mycorpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "## Cleaning and tokenising\n",
                "\n",
                "We start by cleaning the corpus data by removing tags, artefacts and non-alpha-numeric characters.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ace_clean <- # remove A01 0001 1 sequences\n",
                "  stringr::str_remove_all(ace, \"[A-Z][0-9]{2,2}\") %>%\n",
                "  # remove tags \n",
                "  stringr::str_remove_all(\"<.*?>\") %>%\n",
                "  # remove non-alphanumeric characters \n",
                "  stringr::str_remove_all(\"[^[:alnum:] ]\") %>%\n",
                "  # remove single characters\n",
                "  stringr::str_remove_all(\" \\\\w \") %>%\n",
                "  # remove all digits \n",
                "  stringr::str_remove_all(\"\\\\d\") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish()\n",
                "# inspect\n",
                "substr(ace_clean[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Of course, we do the same for the BROWN data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brown_clean <- # remove A01 0001 1 sequences\n",
                "  stringr::str_remove_all(brown, \"[A-Z][0-9]{2,2}\") %>%\n",
                "  # remove tags \n",
                "  stringr::str_remove_all(\"<.*?>\") %>%\n",
                "  # remove non-alphanumeric characters \n",
                "  stringr::str_remove_all(\"[^[:alnum:] ]\") %>%\n",
                "  # remove single characters\n",
                "  stringr::str_remove_all(\" \\\\w \") %>%\n",
                "  # remove all digits \n",
                "  stringr::str_remove_all(\"\\\\d\") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish()\n",
                "# inspect\n",
                "substr(brown_clean[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now split the clean corpora into individual words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ace_words <- quanteda::tokenize_fastestword(ace_clean)\n",
                "brown_words <- quanteda::tokenize_fastestword(brown_clean)\n",
                "# inspect\n",
                "str(ace_words)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting bigrams\n",
                "\n",
                "Now, that we have tokenised the corpus, we can extract n-grams\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create data frame\n",
                "ace_bigrams <- quanteda::tokens_ngrams(tokens(ace_words)) %>%\n",
                "  unlist() %>%\n",
                "  tolower() %>%\n",
                "  table()\n",
                "# inspect\n",
                "head(ace_bigrams, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create table with n-grams\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create data frame\n",
                "ace_bigram_tb <- data.frame(ace_bigrams) %>%\n",
                "  dplyr::rename(bigram = 1,\n",
                "                freq = 2) %>%\n",
                "  dplyr::arrange(-freq) %>%\n",
                "  dplyr::mutate(corpus = \"ace\")\n",
                "# inspect\n",
                "head(ace_bigram_tb, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we will do the same for the brown corpus data\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brown_bigram_tb <- quanteda::tokens_ngrams(tokens(brown_words)) %>%\n",
                "  unlist() %>%\n",
                "  tolower() %>%\n",
                "  table() %>%\n",
                "  data.frame() %>%\n",
                "  dplyr::rename(bigram = 1,\n",
                "                freq = 2) %>%\n",
                "  dplyr::arrange(-freq) %>%\n",
                "  dplyr::mutate(corpus = \"brown\")\n",
                "# inspect\n",
                "head(brown_bigram_tb, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now combine the two tables\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bigram_tb <-dplyr::full_join(ace_bigram_tb, brown_bigram_tb) %>%\n",
                "  dplyr::filter(freq > 10) %>%\n",
                "  tidyr::spread(corpus, freq) %>%\n",
                "  dplyr::mutate(ace = tidyr::replace_na(ace, 0),\n",
                "                brown = tidyr::replace_na(brown, 0),\n",
                "                ace_all = sum(ace),\n",
                "                brown_all = sum(brown)) %>%\n",
                "  dplyr::rowwise() %>%\n",
                "  dplyr::mutate(p = fisher.test(matrix(c(ace, brown, ace_all-ace, brown_all-brown), byrow = T, nrow = 2))[[1]],\n",
                "                oddsratio = fisher.test(matrix(c(ace, brown, ace_all-ace, brown_all-brown), byrow = T, nrow = 2))[[3]]) %>%\n",
                "  dplyr::filter(p < .05/nrow(.)) %>%\n",
                "  dplyr::arrange(p)\n",
                "# inspect\n",
                "head(bigram_tb, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating a Corpus\n",
                "\n",
                "We start by creating a table of the ACE data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ace_df <- data.frame(rep(\"ACE\", length(acefiles)), acefiles, ace) %>%\n",
                "  # change column names\n",
                "  dplyr::rename(corpus = 1, \n",
                "                file = 2,\n",
                "                content = 3) %>%\n",
                "  # shorten file names\n",
                "  dplyr::mutate(file = stringr::str_remove_all(file, \".*/\"),\n",
                "                file = stringr::str_remove_all(file, \".TXT\")) %>%\n",
                "  # clean corpus data\n",
                "  dplyr::mutate(content = ace_clean)\n",
                "# inspect\n",
                "substr(ace_df[1, 3], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We also create a table of the Brown data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brown_df <- data.frame(rep(\"BROWN\", length(brownfiles)), brownfiles, brown) %>%\n",
                "  # change column names\n",
                "  dplyr::rename(corpus = 1, \n",
                "                file = 2,\n",
                "                content = 3) %>%\n",
                "  # shorten file names\n",
                "  dplyr::mutate(file = stringr::str_remove_all(file, \".*/\"),\n",
                "                file = stringr::str_remove_all(file, \".TXT\")) %>%\n",
                "  # clean corpus data\n",
                "  dplyr::mutate(content = brown_clean)\n",
                "# inspect\n",
                "substr(brown_df[1, 3], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now combine the two data frames into one.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- rbind(ace_df, brown_df)\n",
                "# inspect\n",
                "str(corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comparing the corpora \n",
                "\n",
                "There are many ways in which we can compare corpora. Here, we will  extract keywords that are characteristic for each of the two corpora and then visualize them in two different ways: in the form a bar graphs and as comparative word clouds.\n",
                "\n",
                "## Extracting Keywords \n",
                "\n",
                "When visualizing keywords in bar graphs, we first need to extract the keywords. this has the advantage that we can tabulate the keywords and inspect them before visualizing them.\n",
                "\n",
                "To extract the keywords, we combine the two corpora \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only select speeches by Obama and Trump\n",
                "comp_corpus <- quanteda::corpus(corpus$content, docvars = data.frame(file = corpus$file,\n",
                "                                                                  corpus = corpus$corpus))\n",
                "# inspect\n",
                "str(comp_corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we convert the data into a document-frequency matrix (dfm) which shows how frequent words are in each document.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dfm grouped by president\n",
                "corp_dfm <- tokens(comp_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%\n",
                "  tokens_remove(stopwords(\"english\")) %>%\n",
                "  tokens_group(groups = corpus) %>%\n",
                "  dfm()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we calculate the keyness of words in the corpora.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculate keyness and determine Trump as target group\n",
                "result_keyness <- textstat_keyness(corp_dfm, \n",
                "                                   target = \"ACE\")\n",
                "# inspect\n",
                "head(result_keyness)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can then visualize the keywords as a bar graph using the `textplot_keyness` function that is provided in the `quanteda.textplots`  package.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot estimated word keyness\n",
                "quanteda.textplots::textplot_keyness(result_keyness) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparative Wordclouds \n",
                "\n",
                "\n",
                "We can use the `textplot_wordcloud` function to generate not only simple wordclouds but also comparative wordclouds that can be use to compare corpora.\n",
                "\n",
                "To generate a comparative wordcloud, you need to set the argument`comparison` to `TRUE` and we can specify  arguments to *prettify* the wordcloud. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# clean corpus\n",
                "corp_dfm %>%\n",
                "  # create word cloud\n",
                "  quanteda.textplots::textplot_wordcloud(comparison = TRUE, max_words = 100, rotation = .25)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Finding collocations\n",
                "\n",
                "There are various techniques for identifying collocations. To identify collocations without having a pre-defined target term, we can use the `textstat_collocations` function from the `quanteda.textstats` package.\n",
                "\n",
                "However, before we can apply that function and start identifying collocations, we need to process the data to which we want to apply this function. In the present case, we will apply that function to the sentences in the example text which we extract in the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ace_sentences <- ace %>%\n",
                "  stringr::str_replace_all(\"[A-Z][0-9]{2,2}\", \" \") %>%\n",
                "  tolower() %>%\n",
                "  # remove tags \n",
                "  stringr::str_replace_all(\"<.*?>\", \" \") %>%\n",
                "  # remove single characters\n",
                "  stringr::str_replace_all(\" \\\\w{1,2} \", \" \") %>%\n",
                "  # remove all digits \n",
                "  stringr::str_replace_all(\"\\\\d\", \" \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish()%>%\n",
                "  paste0(collapse= \" \") %>%\n",
                "  stringr::str_split(fixed(\".\")) %>%\n",
                "  unlist() %>%\n",
                "  stringr::str_remove_all(\"[^[:alnum:] ]\") %>%\n",
                "  stringr::str_squish()\n",
                "# inspect\n",
                "head(ace_sentences)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the output shown above, we also see that splitting texts simply by full stops is not optimal as it produces some unwarranted artifacts like the “sentences” that consist of single characters (due to the name of the H.M.S. Beagle - the ship on which Darwin traveled when he explored the southern hemisphere). Fortunately, these errors do not really matter in the case of our example.\n",
                "\n",
                "Now that we have split the example text into sentences, we can tokenize these sentences and apply the `textstat_collocations` function which identifies collocations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a token object\n",
                "text_tokens <- tokens(ace_sentences, remove_punct = TRUE) %>%\n",
                "  tokens_remove(stopwords(\"english\"))\n",
                "# extract collocations\n",
                "text_coll <- textstat_collocations(text_tokens, size = 2, min_count = 20)\n",
                "# inspect\n",
                "text_coll[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The resulting table shows collocations in the example text descending by collocation strength.\n",
                "\n",
                "\n",
                "## Visualizing Collocation Networks\n",
                "\n",
                "Network graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term alice using the quanteda package.\n",
                "\n",
                "In a first step, we generate a document-feature matrix based on the sentences in the example text. A document-feature matrix shows how often elements (here these elements are the words that occur in the the example text) occur in a selection of documents (here these documents are the sentences in the example text).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_dfm <- ace_sentences %>% \n",
                "    quanteda::dfm(remove = stopwords('english'), remove_punct = TRUE) %>%\n",
                "    quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)\n",
                "# inspect\n",
                "text_dfm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# create document-feature matrix\n",
                "\n",
                "As we want to generate a network graph of words that collocate with the term organism, we use the `calculateCoocStatistics` function to determine which words most strongly collocate with our target term (organism).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function for co-occurrence calculation\n",
                "source(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n",
                "# define term\n",
                "coocTerm <- \"australia\"\n",
                "# calculate co-occurrence statistics\n",
                "coocs <- calculateCoocStatistics(coocTerm, text_dfm, measure=\"LOGLIK\")\n",
                "# inspect results\n",
                "coocs[1:20]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now reduce the document-feature matrix to contain only the top 20 collocates of organism (plus our target word organism).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "redux_dfm <- dfm_select(text_dfm, \n",
                "                        pattern = c(names(coocs)[1:20], \"australia\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tag_fcm <- fcm(redux_dfm)\n",
                "# inspect\n",
                "tag_fcm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term *australia* with the edges representing the co-occurrence frequency. To generate this network graph, we use the `textplot_network` function from the `quanteda.textplots` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate network graph\n",
                "textplot_network(tag_fcm, \n",
                "                 min_freq = 5, \n",
                "                 edge_alpha = 0.1, \n",
                "                 edge_size = 5,\n",
                "                 edge_color = \"purple\",\n",
                "                 vertex_labelsize = log(colSums(tag_fcm)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Keyness\n",
                "\n",
                "Another common method that can be used for automated text summarization is keyword extraction. Keyword extraction builds on identifying words that are particularly associated with a certain text. In other words, keyness analysis aims to identify words that are particularly indicative of the content of a certain text.\n",
                "\n",
                "Below, we identify key words for Charles Darwin’s Origin, Herman Melville’s Moby Dick, and George Orwell’s 1984. We start by creating a weighted document feature matrix from the corpus containing the three texts.\n",
                "\n",
                "In order to create a corpus, we use the text objects that consist out of many different elements rather than the objects which contained the collapsed texts that we used above. Thus, in a first step, we create a corpus of the texts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom <- quanteda::corpus(c(ace_clean, brown_clean)) \n",
                "attr(corp_dom, \"docvars\")$Corpus = c(rep(\"ACE\", length(ace_clean)), \n",
                "                                     rep(\"BROWN\", length(brown_clean)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we generate the document feature matrix and we clean it by removing stopwords and selected other words. In addition, we group the documents feature matrix by author.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfm_corpus <- corp_dom %>%\n",
                "  quanteda::tokens(remove_punct = TRUE) %>%\n",
                "  quanteda::tokens_remove(quanteda::stopwords(\"english\")) %>%\n",
                "  quanteda::tokens_remove(c(\"now\", \"one\", \"like\", \"may\", \"can\", \"said\", \"even\")) %>%\n",
                "  quanteda::dfm() %>%\n",
                "  quanteda::dfm_group(groups = Corpus) %>%\n",
                "  quanteda::dfm_weight(scheme = \"prop\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we use the `textstat_frequency` function from the `quanteda` package to extract the most frequent non-stopwords in the three texts.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate relative frequency by president\n",
                "freq_weight <- quanteda.textstats::textstat_frequency(dfm_corpus, \n",
                "                                                      n = 10,\n",
                "                                                      groups = dfm_corpus$Corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can simply plot the most common words and most indicative non-stop words in the three texts.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(freq_weight, aes(nrow(freq_weight):1, frequency)) +\n",
                "     geom_point() +\n",
                "     facet_wrap(~ group, scales = \"free\") +\n",
                "     coord_flip() +\n",
                "     scale_x_continuous(breaks = nrow(freq_weight):1,\n",
                "                        labels = freq_weight$feature) +\n",
                "     labs(x = NULL, y = \"Relative frequency\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Outro\n",
                "\n",
                "\n",
                "We end the session by calling the session info which tells us what packages and what version of the software and packages we have used.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to HOME](https://github.com/MartinSchweinberger/SLAT7829Tutorials)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
