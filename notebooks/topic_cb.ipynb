{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](https://github.com/MartinSchweinberger/SLAT7829/blob/master/images/bannerSLAT7829.jpeg?raw=true)\n",
                "\n",
                "# Topic modelling\n",
                "\n",
                "This tutorial show how you can perform topic modeling.\n",
                "\n",
                "However, please keep in mind that the case studies  merely aim to exemplify ways in which R can be used in language-based research - rather than providing detailed procedures on how to do corpus-based research. \n",
                "\n",
                "Topic modeling is a technique used in data analysis to identify and extract topics or themes from a large collection of texts. It is a way to automatically identify patterns and insights in large amounts of unstructured data.\n",
                "\n",
                "Topic modeling is useful in a variety of fields, such as social sciences, humanities, and marketing research. For example, it can be used to analyze customer reviews to identify common themes, to study political speeches to identify key issues and topics, or to analyze social media data to understand public opinion on a particular topic.\n",
                "\n",
                "The technique works by analyzing the frequency of words that appear in a text corpus and grouping them into topics that frequently co-occur. These topics can then be interpreted and labeled based on the words that are most strongly associated with them. The result is a set of topics that represent the most important themes in the text corpus.\n",
                "\n",
                "## Preparation and session set up\n",
                "\n",
                "Activate required packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(dplyr)\n",
                "library(tidyr)\n",
                "library(stringr)\n",
                "library(ggplot2)\n",
                "library(quanteda)\n",
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n",
                "library(seededlda)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading data\n",
                "\n",
                "To analyze n-grams and collocations, we first need to load some data. In this tutorial, we will use the essays written by German and Spanish learners of English provided in the International Corpus of Learner English (ICLE).\n",
                "\n",
                "Loading corpus data into R consists of two steps: \n",
                "\n",
                "1. create a list of paths of the corpus files\n",
                "\n",
                "2. loop over these paths and load the data in the files identified by the paths.\n",
                "\n",
                "To create a list of corpus files, you could use the code chunk below (the code chunk assumes that the ICLE data is in a folder called *ICLE*).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load ace files\n",
                "corpusfiles <- list.files(here::here(\"ICLE\"), # path to the corpus data\n",
                "                       pattern = \"GE|SP\",\n",
                "                       # full paths - not just the names of the files\n",
                "                       full.names = T) \n",
                "# load the files by scanning the content\n",
                "corpus <- sapply(corpusfiles, function(x){\n",
                "  x <- scan(x, what = \"char\",  sep = \"\", quote = \"\",  quiet = T,  skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "You can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "Then, click on the `New Folder` symbol and create a new folder and call it `MyData`.\n",
                "\n",
                "![Binder New Folder Symbol](https://slcladal.github.io/images/bindernewfolder.JPG)\n",
                "\n",
                "Then click on the upload symbol and upload your files into the `MyData` folder.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT**: here, we assume that you upload some form of text data - not tabular data!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load files\n",
                "mycorpus <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mycorpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "## Cleaning and tokenising\n",
                "\n",
                "We start by cleaning the corpus data by removing tags, artefacts and non-alpha-numeric characters.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus_clean <- # remove A01 0001 1 sequences\n",
                "  stringr::str_remove_all(corpus, \"<.*?>\") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish()\n",
                "# inspect\n",
                "substr(corpus_clean[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now split the clean corpora into individual words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "toks_corpus <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)\n",
                "toks_corpus <- tokens_remove(toks_corpus, pattern = c(stopwords(\"en\"), \"*-time\", \"updated-*\", \"gmt\", \"bst\"))\n",
                "dfmat_corpus <- dfm(toks_corpus) %>% \n",
                "              dfm_trim(min_termfreq = 0.8, termfreq_type = \"quantile\",\n",
                "                       max_docfreq = 0.1, docfreq_type = \"prop\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Unsupervised LDA\n",
                "\n",
                "change k to set different N of topics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set seed\n",
                "set.seed(1234)\n",
                "# generate model\n",
                "tmod_lda <- seededlda::textmodel_lda(dfmat_corpus, k = 15)\n",
                "# inspect\n",
                "terms(tmod_lda, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Supervised LDA\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# semisupervised LDA\n",
                "dict <- dictionary(list(Computer = c(\"computers\", \"information\", \"machine\", \"computer\"),\n",
                "                        Education = c(\"students\", \"courses\", \"education\", \"university\"),\n",
                "                        Movies = c(\"movie\", \"film\", \"commercial\", \"watch\"),\n",
                "                        Family = c(\"parents\", \"home\", \"mother\", \"father\"),\n",
                "                        War = c(\"war\", \"peace\", \"somalia\", \"consequences\"),\n",
                "                        Foreigners = c(\"foreigners\", \"germans\", \"turkish\", \"turks\"),\n",
                "                        Phone = c(\"phone\", \"telephone\", \"call\"),\n",
                "                        Food = c(\"mcdonald's\", \"restaurant\", \"chips\", \"fastfood\", \"taste\"),\n",
                "                        Pets = c(\"dog*\", \"walk\", \"cat\", \"pet\", \"happy\"),\n",
                "                        Eco = c(\"green\", \"car*\", \"drive\", \"speed\", \"accident\", \"exhaust\"),\n",
                "                        Dating = c(\"girl\", \"boy\", \"date\", \"merries\")))\n",
                "tmod_slda <- textmodel_seededlda(dfmat_corpus, dict, residual = TRUE, min_termfreq = 10)\n",
                "terms(tmod_slda)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "inspect topic by file\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "topics(tmod_slda)[1:20]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "extract files and create a data frame of topics and documents \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "files <- stringr::str_replace_all(names(topics(tmod_slda)), \".*/(.*?).txt\", \"\\\\1\")\n",
                "topics <- topics(tmod_slda)\n",
                "language <- ifelse(stringr::str_detect(files, \"GE\"), \"German\", \"Spanish\")\n",
                "df <- data.frame(language, topics) %>%\n",
                "  dplyr::mutate_if(is.character, factor)\n",
                "# inspect\n",
                "head(df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "calculate percentages\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfp <- df %>%\n",
                "  dplyr::group_by(language, topics) %>%\n",
                "  dplyr::summarise(freq = n()) %>%\n",
                "  dplyr::group_by(language) %>%\n",
                "  dplyr::mutate(all = sum(freq),\n",
                "                percent = round(freq/all*100, 2))\n",
                "# inspect\n",
                "head(dfp)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfp %>%\n",
                "  ggplot(aes(x = topics, y = percent, label = percent, fill = language)) +\n",
                "  geom_bar(stat = \"identity\", position = position_dodge()) + \n",
                "  geom_text(vjust=-0.3, position = position_dodge(0.9)) + \n",
                "  theme_bw() +\n",
                "  coord_cartesian(ylim = c(0, 30)) +\n",
                "  labs(x = \"Topic\", y = \"Percent\") +\n",
                "  theme(legend.position = \"top\",\n",
                "        axis.text.x = element_text(angle = 90))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Outro\n",
                "\n",
                "\n",
                "We end the session by calling the session info which tells us what packages and what version of the software and packages we have used.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to HOME](https://github.com/MartinSchweinberger/SLAT7829Tutorials)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
