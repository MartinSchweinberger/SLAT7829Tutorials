{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](https://github.com/MartinSchweinberger/SLAT7829/blob/master/images/bannerSLAT7829.jpeg?raw=true)\n",
                "\n",
                "# Exploring Corpus Data\n",
                "\n",
                "This tutorial showcases how to perform basic corpus linguistics methods to explore language data using the *Australian Corpus of English* (ACE). The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with corpus linguistics.\n",
                "\n",
                "Most of the applications of Corpus Linguistics are based upon a relatively limited number of key procedures or concepts (e.g. concordancing, word frequencies, annotation or tagging, parsing, collocation, text classification, Sentiment Analysis, Entity Extraction, Topic Modeling, etc.). In the following, we will explore some of these procedures to help you perform such tasks. \n",
                "\n",
                "\n",
                "## Preparation and session set up  \n",
                "\n",
                "\n",
                "Activate required packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(dplyr)\n",
                "library(stringr)\n",
                "library(ggplot2)\n",
                "library(quanteda)\n",
                "library(tm)\n",
                "library(tidytext)\n",
                "library(wordcloud2)\n",
                "library(flextable)\n",
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n",
                "library(tidyr)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have  initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "# Loading data\n",
                "\n",
                "\n",
                "\n",
                "In the following, we will use R to query and investigate the *Australian Corpus of English* (ACE). \n",
                "\n",
                "We begin by loading the data which consists of two steps: (1) creating a vector or list of all files that we want to load and then (2) loading the files.\n",
                "\n",
                "We start by creating a list of files we want to load.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "corpusfiles <- list.files(here::here(\"ACE\"), # path to the corpus data\n",
                "                          # file types you want to analyze, e.g. txt-files\n",
                "                          pattern = \".*.TXT\",\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# inspect\n",
                "head(corpusfiles)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we load the data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- sapply(corpusfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "You can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "Then, click on the `New Folder` symbol and create a new folder and call it `MyData`.\n",
                "\n",
                "![Binder New Folder Symbol](https://slcladal.github.io/images/bindernewfolder.JPG)\n",
                "\n",
                "Then click on the upload symbol and upload your files into the `MyData` folder.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT**: here, we assume that you upload some form of text data - not tabular data!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mycorpus <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mycorpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "# Cleaning data\n",
                "\n",
                "We start by cleaning the corpus data by removing tags, artefacts and non-alpha-numeric characters.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- # remove A01 0001 1 sequences\n",
                "  stringr::str_remove_all(corpus, \"[A-Z][0-9]{2,2}\") %>%\n",
                "  # remove tags \n",
                "  stringr::str_remove_all(\"<.*?>\") %>%\n",
                "  # remove non-alphanumeric characters \n",
                "  stringr::str_remove_all(\"[^[:alnum:] ]\") %>%\n",
                "  # remove single characters\n",
                "  stringr::str_remove_all(\" \\\\w \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish()\n",
                "# inspect\n",
                "substr(corpus[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Concordancing\n",
                "\n",
                "Once we have loaded and cleaned the data, we perform the concordancing and extract the KWICs. \n",
                "\n",
                "Concordancing refers to the extraction of words from a given text or texts. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. \n",
                "\n",
                "Concordancing is helpful for seeing how the term is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure and often the first step in more sophisticated analyses of language data. \n",
                "\n",
                "To create these kwics, we use the `kwic` function from the `quanteda` package. \n",
                "\n",
                "\n",
                "The `kwic` function has the following schema:\n",
                "\n",
                "`kwic(x, pattern, window = 5, valuetype = c(\"glob\", \"regex\", \"fixed\"), separator = \" \", case_insensitive = TRUE, index = NULL)`\n",
                "\n",
                "The arguments (or parameters) of the `kwic` function mean:\n",
                "\n",
                "* `x`: a character, corpus, or tokens object  \n",
                "* `pattern`: a character vector, list of character vectors, dictionary, or collocations object. See pattern for details.  \n",
                "* `window`: the number of context words to be displayed around the keyword  \n",
                "* `valuetype`: the type of pattern matching: \"glob\" for \"glob\"-style wildcard expressions; \"regex\" for regular expressions; or \"fixed\" for exact matching. See valuetype for details.  \n",
                "* `separator`: a character to separate words in the output  \n",
                "* `case_insensitive`: logical; if TRUE, ignore case when matching a pattern or dictionary values  \n",
                "* `index`: an index object to specify keywords\n",
                "\n",
                "\n",
                "\n",
                "To start with, we generate kwics for the term *australia* as shown below. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "australia <- lapply(corpus, function(x){\n",
                "  x <- quanteda::kwic(tokens(x), # define text(s) \n",
                "                         # define pattern\n",
                "                          pattern = \"australia\",\n",
                "                         # define window size\n",
                "                          window = 5) %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n",
                "})\n",
                "# combine list of data frames into one big data frame\n",
                "australia <- dplyr::bind_rows(australia)\n",
                "# inspect data\n",
                "head(australia)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also use regular expressions in our search to extract not only *australia* but also more complex and even vague patterns. Vague means that only part of the pattern is specified. For instance, maybe only *walk* is specified and now we want all words containing this sequence including *walking*, *walker*, *walked*, and *walks*. To retrieve such vague patterns, we need to use so-called *regular expressions*. Also, when using a regular expression in the `pattern` argument, we need to specify the `valuetype` as `regex` (as shown below).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "austr <- lapply(corpus, function(x){\n",
                "  x <- quanteda::kwic(x = tokens(x), \n",
                "                          pattern = \"austr.*\",\n",
                "                          window = 5,\n",
                "                          valuetype = \"regex\") %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n",
                "})\n",
                "# combine list of data frames into one big data frame\n",
                "austr <- dplyr::bind_rows(austr)\n",
                "# inspect data\n",
                "head(austr)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When search for expressions that represent phrase and that consists out of several elements such as *new south wales*, we also need to specify that we are looking for a phrase in the pattern argument. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "nsw <- lapply(corpus, function(x){\n",
                "  x <- quanteda::kwic(x = tokens(x), \n",
                "                          pattern = quanteda::phrase(\"new south wales\"),\n",
                "                          window = 5) %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # clean file names\n",
                "    dplyr::mutate(docname = stringr::str_remove_all(docname, \".*/\"),\n",
                "                docname = stringr::str_remove_all(docname, \".TXT\"))\n",
                "})\n",
                "# combine list of data frames into one big data frame\n",
                "nsw <- dplyr::bind_rows(nsw)\n",
                "# inspect data\n",
                "head(nsw)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We could now continue and analyze how certain words and phrases are used.\n",
                "\n",
                "# Word Frequency\n",
                "\n",
                "Almost all methods used in text analytics rely on frequency information. Thus, fending out out frequent words are in a text is a fundamental technique in text analytics. In fact, frequency information lies at the very core of Text Analysis. Such frequency information often comes in the form of word frequency lists, i.e. lists of word forms and their frequency in a given text or collection of texts.  \n",
                "\n",
                "As extracting word frequency lists is very important, we will now We will now extract a frequency list from a corpus.\n",
                "\n",
                "In a first step, we load a corpus, convert everything to lower case, remove non-word symbols (including punctuation), and split the corpus data into individual words.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and process corpus\n",
                "ace_words <- corpus  %>%\n",
                "  # remove tags\n",
                "  stringr::str_remove_all(\"<.*?<\/.*?>\") %>%\n",
                "  # convert everything to lower case\n",
                "  tolower() %>%\n",
                "  # remove non-word characters\n",
                "  str_replace_all(\"[^[:alpha:][:space:]]*\", \"\")  %>%\n",
                "  tm::removePunctuation() %>%\n",
                "  stringr::str_squish() %>%\n",
                "  stringr::str_split(\" \") %>%\n",
                "  unlist()\n",
                "# inspect data\n",
                "head(ace_words)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have a vector of words, we can easily create a table representing a word frequency list (as shown below).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table\n",
                "wfreq <- ace_words %>%\n",
                "  table() %>%\n",
                "  as.data.frame() %>%\n",
                "  arrange(desc(Freq)) %>%\n",
                "  dplyr::rename(word = 1,\n",
                "                frequency = 2)\n",
                "# inspect data\n",
                "head(wfreq, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The most frequent words are all function words which are often not meaningful or useful for an analysis. Thus, we now remove these function words (also called *stopwords*) from the frequency list and inspect the list without stopwords.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table wo stopwords\n",
                "wfreq_wostop <- wfreq %>%\n",
                "  anti_join(tidytext::stop_words, by = \"word\") %>%\n",
                "  dplyr::filter(word != \"\")\n",
                "# inspect data\n",
                "head(wfreq_wostop, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Such word frequency lists can be visualized in various ways. The most common way to visualize word frequency lists is in the form of bargraphs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wfreq_wostop %>%\n",
                "  head(10) %>%\n",
                "  ggplot(aes(x = reorder(word, -frequency, mean), y = frequency)) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  labs(title = \"10 most frequent non-stop words \\nin the example text\",\n",
                "       x = \"\") +\n",
                "  theme(axis.text.x = element_text(angle = 45, size = 12, hjust = 1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Wordclouds\n",
                "\n",
                "Alternatively, word frequency lists can be visualized, although less informative, as word clouds. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create wordcloud\n",
                "corpus %>%\n",
                "  # remove tags\n",
                "  stringr::str_remove_all(\"<.*?<\/.*?>\") %>%\n",
                "  # convert to corpus\n",
                "  quanteda::corpus() %>%\n",
                "  # remove punctuation\n",
                "  quanteda::tokens(remove_punct = TRUE,\n",
                "                   remove_symbols = TRUE,\n",
                "                   remove_numbers = TRUE) %>%\n",
                "  # remove stop words\n",
                "  quanteda::tokens_remove(stopwords(\"english\")) %>%\n",
                "  # create data frequency matrix\n",
                "  quanteda::dfm() %>%\n",
                "  # create word cloud\n",
                "  quanteda.textplots::textplot_wordcloud(max_words = 200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `textplot_wordcloud` function has the following schema:\n",
                "\n",
                "`textplot_wordcloud(x, min_size = 0.5, max_size = 4, min_count = 3, max_words = 500, color = \"darkblue\",`\n",
                "  `font = NULL, adjust = 0, rotation = 0.1, random_order = FALSE, random_color = FALSE,`\n",
                "  `ordered_color = FALSE, labelcolor = \"gray20\", labelsize = 1.5, labeloffset = 0, `\n",
                "  `fixed_aspect = TRUE, comparison = FALSE )`\n",
                "\n",
                "The arguments (or parameters) of the `kwic` function mean:\n",
                "\n",
                "* `x`: a dfm or quanteda.textstats::textstat_keyness object  \n",
                "* `min_size`: size of the smallest word  \n",
                "* `max_size`: size of the largest word  \n",
                "* `min_count`: words with frequency below min_count will not be plotted  \n",
                "* `max_words`: maximum number of words to be plotted. The least frequent terms dropped. The maximum frequency will be split evenly across categories when comparison = TRUE.  \n",
                "* `color`: colour of words from least to most frequent  \n",
                "* `font`: font-family of words and labels. Use default font if NULL.  \n",
                "* `adjust`: adjust sizes of words by a constant. Useful for non-English words for which R fails to obtain correct sizes.  \n",
                "* `rotation`: proportion of words with 90 degree rotation  \n",
                "* `random_order`: plot words in random order. If FALSE, they will be plotted in decreasing frequency.  \n",
                "* `random_color`: choose colours randomly from the colours. If FALSE, the colour is chosen based on the frequency  \n",
                "* `ordered_color`: if TRUE, then colours are assigned to words in order.  \n",
                "* `labelcolor`: colour of group labels. Only used when comparison = TRUE.  \n",
                "* `labelsize`: size of group labels. Only used when comparison = TRUE.  \n",
                "* `labeloffset`: position of group labels. Only used when comparison = TRUE.  \n",
                "* `fixed_aspect`: logical; if TRUE, the aspect ratio is fixed. Variable aspect ratio only supported if rotation = 0.  \n",
                "* `comparison`: logical; if TRUE, plot a wordcloud that compares documents in the same way as wordcloud::comparison.cloud(). If x is a quanteda.textstats::textstat_keyness object, then only the target category's key terms are plotted when comparison = FALSE, otherwise the top max_words / 2 terms are plotted from the target and reference categories.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create wordcloud\n",
                "corpus %>%\n",
                "  stringr::str_remove_all(\"<.*?<\/.*?>\") %>%\n",
                "  quanteda::corpus() %>%\n",
                "  quanteda::tokens(remove_punct = TRUE,\n",
                "                   remove_symbols = TRUE,\n",
                "                   remove_numbers = TRUE) %>%\n",
                "  quanteda::tokens_remove(stopwords(\"english\")) %>%\n",
                "  quanteda::dfm() %>%\n",
                "   dfm_trim(min_termfreq = 5) %>%\n",
                "  quanteda.textplots::textplot_wordcloud(max_words = 200, rotation = .25,  color = RColorBrewer::brewer.pal(10, \"RdBu\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analyzing Frequencies \n",
                "\n",
                "We can also investigate the use of the term *australia* across chapters of the example text. In a first step, we extract the number of words in each chapter.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract number of words per chapter\n",
                "Words <- corpus %>%\n",
                "  stringr::str_split(\" \")  %>%\n",
                "  lengths()\n",
                "# inspect data\n",
                "Words\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we extract the number of matches in each chapter.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract number of matches per chapter\n",
                "Matches <- corpus %>%\n",
                "  tolower() %>%\n",
                "  stringr::str_count(\"australia\")\n",
                "# inspect the number of matches per chapter\n",
                "Matches\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract chapters\n",
                "corpusfiles <- stringr::str_remove_all(corpusfiles, \".*/\") %>%\n",
                "  stringr::str_remove_all(\".TXT\")\n",
                "corpusfiles\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table of results\n",
                "tb <- data.frame(corpusfiles, Matches, Words) %>%\n",
                "  dplyr::mutate(Frequency = round(Matches/Words*1000, 2))\n",
                "# inspect data\n",
                "tb\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now visualize the relative frequencies of our search word per chapter.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create plot\n",
                "ggplot(tb, aes(x = reorder(corpusfiles, -Frequency), y = Frequency)) + \n",
                "  geom_bar(stat = \"identity\") + \n",
                "  geom_text(aes(y = Frequency +.5, label = Frequency), color = \"gray30\", size=4) + \n",
                "  theme_bw() +\n",
                "  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n",
                "  labs(y =\"Relative Frequency (per 1,000 words)\", x = \"Text type in ACE\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dispersion plots\n",
                "\n",
                "To show when in a text or in a collection of texts certain terms occur, we can use *dispersion plots*. The `quanteda` package offers a very easy-to-use function `textplot_xray` to generate dispersion plots.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add chapter names\n",
                "names(corpus) <- corpusfiles\n",
                "# generate dispersion plots\n",
                "quanteda.textplots::textplot_xray(kwic(tokens(corpus), pattern = \"australia\"),\n",
                "                                  kwic(tokens(corpus), pattern = \"queensland\"),\n",
                "                                  sort = T)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can modify the plot by saving it into an object and then use `ggplot` to modify it appearance.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate and save dispersion plots\n",
                "dp <- quanteda.textplots::textplot_xray(kwic(tokens(corpus), pattern = \"queensland\"),\n",
                "                                        kwic(tokens(corpus), pattern = phrase(\"new south wales\")))\n",
                "# modify plot\n",
                "dp + aes(color = keyword) + \n",
                "  scale_color_manual(values = c('red', 'blue')) +\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We end the session by calling the session info which tells us what packages and what version of the software and packages we have used.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to HOME](https://github.com/MartinSchweinberger/SLAT7829Tutorials)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
