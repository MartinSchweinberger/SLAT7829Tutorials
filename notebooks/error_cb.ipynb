{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](https://github.com/MartinSchweinberger/SLAT7829/blob/master/images/bannerSLAT7829.jpeg?raw=true)\n",
                "\n",
                "# Case Study: Detecting errors in student writing\n",
                "\n",
                "This tutorial show how you can detect errors in student essays based on a subsample of the *International Corpus of Learner English* (ICLE).\n",
                "\n",
                "This case study represents a corpus-based study of orthographic errors (not grammatical or stylistic errors).The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with corpus linguistics.\n",
                "\n",
                "## Preparation and session set up\n",
                "\n",
                "Activate required packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(dplyr)\n",
                "library(stringr)\n",
                "library(ggplot2)\n",
                "library(quanteda)\n",
                "library(quanteda.textstats)\n",
                "library(hunspell)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading the corpus data\n",
                "\n",
                "Loading corpus data consists of two steps: \n",
                "\n",
                "1. create a list of paths of the corpus files\n",
                "\n",
                "2. loop over these paths and load the data in the files identified by the paths.\n",
                "\n",
                "To create a list of corpus files, you could use the code chunk below (the code chunk assumes that the corpus data is in a folder called *Corpus* in the *data* sub-folder of your Rproject folder).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpusfiles <- list.files(here::here(\"ICLE\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# inspect\n",
                "head(corpusfiles)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can then use the `sapply` function to loop over the paths and load the data int R using e.g. the `scan` function as shown below. In addition to loading the file content, we also paste all the content together using the `paste0` function and remove superfluous white spaces using the `str_squish` function from the `stringr` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- sapply(corpusfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have loaded your data into R, you can then continue with processing and transforming the data according to your needs.\n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "You can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "Then, click on the `New Folder` symbol and create a new folder and call it `MyData`.\n",
                "\n",
                "![Binder New Folder Symbol](https://slcladal.github.io/images/bindernewfolder.JPG)\n",
                "\n",
                "Then click on the upload symbol and upload your files into the `MyData` folder.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT**: here, we assume that you upload some form of text data - not tabular data!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mycorpus <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mycorpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "## Data processing\n",
                "\n",
                "\n",
                "Now that the corpus data is loaded, we extract the file names.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filenames <- names(corpus) %>%\n",
                "  stringr::str_remove_all(\".*/\")  %>%\n",
                "  stringr::str_remove_all(\"\\\\..*\")\n",
                "# inspect\n",
                "head(filenames)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we can clean the data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- corpus %>%\n",
                "  stringr::str_remove_all(\"<.*?>\") %>%\n",
                "  stringr::str_squish()\n",
                "# add names\n",
                "names(corpus) <- filenames\n",
                "# inspect\n",
                "substr(corpus[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we perform the error detection. To do this, we loop over the corpus and use the `hunspell()` function. Within the `hunspell()` function, we can also specify the dictionary (whether we want to use a British or an American English dictionary) via the `dict = ` argument.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors_gb <- sapply(corpus, function(x){\n",
                "  x <- hunspell(x, dict = 'en_GB') })\n",
                "# inspect \n",
                "head(errors_gb)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that many proper nouns (names) are not in the dictionary and thus detected as errors. In addition, we see that the text is written in American English. - so we repeat the analysis with the American English dictionary.\n",
                "\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors_us <- sapply(corpus, function(x){\n",
                "  x <- hunspell(x, dict = 'en_US') })\n",
                "# inspect \n",
                "head(errors_us)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A better way to perform error detection is to create a table of the textual data with each word representing a separate line in the table. This allows you to correct errors detected by the spell checking.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texttable <- lapply(corpus, function(x){\n",
                "  # split text into words\n",
                "  x <- quanteda::tokens(x) %>%\n",
                "    # flatten data\n",
                "  unlist() %>%\n",
                "    # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "    # rename first column\n",
                "  dplyr::rename(words = 1) %>%\n",
                "    # create an id column and performing error correction\n",
                "  dplyr::mutate(id = 1:nrow(.),\n",
                "                error = hunspell::hunspell_check(words, dict = \"en_US\")) %>%\n",
                "  # moving it to first position\n",
                "  dplyr::relocate(id) })\n",
                "# combine list of tables into one table\n",
                "ttable <- do.call(\"rbind\", texttable) %>%\n",
                "  dplyr::mutate(corpus = stringr::str_replace_all(rownames(.), \"(^[A-Z]{2}).*\", \"\\\\1\"))\n",
                "# inspect\n",
                "head(ttable)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now inspect the errors that were detected.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "erors_raw <- ttable %>%\n",
                "  dplyr::filter(error == FALSE)\n",
                "# inspect\n",
                "head(erors_raw, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now correct falsely detected errors.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ttable <- ttable %>%\n",
                "  dplyr::mutate(error = ifelse(str_detect(words, \"\\\\W\"), TRUE, error)) %>%\n",
                "  dplyr::mutate(error = ifelse(str_detect(words, \"^[A-Z]\"), TRUE, error)) %>%\n",
                "  dplyr::mutate(error = ifelse(words == \"Basinger\", TRUE, error))\n",
                "# inspect\n",
                "head(ttable, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now re-inspect the errors that were detected.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors_checked <- ttable %>%\n",
                "  dplyr::filter(error == FALSE)\n",
                "# inspect\n",
                "head(errors_checked, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once the errors are corrected, we can tabulate the errors by language background.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb1 <- ttable %>%\n",
                "  # rename levels of corpus\n",
                "  dplyr::mutate(corpus = case_when(corpus == \"GE\" ~ \"German\",\n",
                "                                   corpus == \"SP\" ~ \"Spanish\",\n",
                "                                   corpus == \"PO\" ~ \"Polish\",\n",
                "                                   corpus == \"RU\" ~ \"Russian\",\n",
                "                                   T ~ corpus)) %>%\n",
                "  # groups by corpus\n",
                "  dplyr::group_by(corpus) %>%\n",
                "  # sum up errors and words\n",
                "  dplyr::summarise(errors = table(error)[1],\n",
                "                   words = n()) %>%\n",
                "# calculate relative frequency\n",
                "  dplyr::mutate(Frequency = round(errors/words*1000, 2))\n",
                "# inspect\n",
                "tb1\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now,as a final step, we can visualize the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb1 %>%\n",
                "  ggplot(aes(x = corpus, y = Frequency)) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  geom_text(aes(y = Frequency-5, label = Frequency), color = \"white\", size=5) +\n",
                "  theme_bw() +\n",
                "  labs(x = \"L1 of student\", y = \"Relative Frequency \\n(orthographic errors per 1,000 words)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We end the session by calling the session info which tells us what packages and what version of the software and packages we have used.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to HOME](https://github.com/MartinSchweinberger/SLAT7829Tutorials)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
