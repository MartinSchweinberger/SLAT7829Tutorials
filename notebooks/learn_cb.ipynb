{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](https://github.com/MartinSchweinberger/SLAT7829/blob/master/images/bannerSLAT7829.jpeg?raw=true)\n",
                "\n",
                "# Case Study: Article Use in ESL \n",
                "\n",
                "This tutorial presents a case study on analyzing article use among L2 learners of English based on a subsample of the *International Corpus of Learner English* (ICLE).\n",
                "\n",
                "This case study represents a corpus-based study of article use and we will test the hypothesis that learners of English of speak a Slavic L1 (which do not have articles in the same way English has) will have more profound difficulties in using articles compared to learners with a German or Spanish language background wholse L1 is similar to English with respect to artcile use.\n",
                "\n",
                "\n",
                "\n",
                "## Preparation and session set up\n",
                "\n",
                "Activate required packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(dplyr)\n",
                "library(stringr)\n",
                "library(ggplot2)\n",
                "library(quanteda)\n",
                "library(udpipe)\n",
                "library(here)\n",
                "library(tidyr)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading the corpus data\n",
                "\n",
                "Loading corpus data consists of two steps: \n",
                "\n",
                "1. create a list of paths of the corpus files\n",
                "\n",
                "2. loop over these paths and load the data in the files identified by the paths.\n",
                "\n",
                "To create a list of corpus files, you could use the code chunk below (the code chunk assumes that the corpus data is in a folder called *Corpus* in the *data* sub-folder of your Rproject folder).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpusfiles <- list.files(here::here(\"ICLE\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# inspect\n",
                "head(corpusfiles)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can then use the `sapply` function to loop over the paths and load the data int R using e.g. the `scan` function as shown below. In addition to loading the file content, we also paste all the content together using the `paste0` function and remove superfluous white spaces using the `str_squish` function from the `stringr` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- sapply(corpusfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have loaded your data into R, you can then continue with processing and transforming the data according to your needs.\n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "You can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "Then, click on the `New Folder` symbol and create a new folder and call it `MyData`.\n",
                "\n",
                "![Binder New Folder Symbol](https://slcladal.github.io/images/bindernewfolder.JPG)\n",
                "\n",
                "Then click on the upload symbol and upload your files into the `MyData` folder.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT**: here, we assume that you upload some form of text data - not tabular data!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mycorpus <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mycorpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "## Data processing\n",
                "\n",
                "\n",
                "Now that the corpus data is loaded, we extract the file names.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filenames <- names(corpus) %>%\n",
                "  stringr::str_remove_all(\".*/\")  %>%\n",
                "  stringr::str_remove_all(\"\\\\..*\")\n",
                "# inspect\n",
                "head(filenames)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we can clean the data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- corpus %>%\n",
                "  stringr::str_remove_all(\"<.*?>\") %>%\n",
                "  stringr::str_squish()\n",
                "# inspect\n",
                "substr(corpus[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we add part of speech tags. To do this, we download the English language model and then load this model into R.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_eng <- udpipe::udpipe_download_model(language = \"english-ewt\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model\n",
                "m_eng <- udpipe_load_model(file = here::here(\"english-ewt-ud-2.5-191206.udpipe\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After loading the language model, we can use it to pos-tag the essays.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_anndf <- udpipe::udpipe_annotate(m_eng, x = corpus[1]) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence)\n",
                "# inspect\n",
                "head(text_anndf, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ger <- corpus[str_detect(filenames, \"^GE\")]\n",
                "spa <- corpus[str_detect(filenames, \"^SP\")]\n",
                "pol <- corpus[str_detect(filenames, \"^PO\")]\n",
                "rus <- corpus[str_detect(filenames, \"^RU\")]\n",
                "# inspect\n",
                "# inspect tagged text\n",
                "substr(ger[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pos-tag L1 German data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "ger_tagged <- udpipe::udpipe_annotate(m_eng, ger) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::group_by(doc_id) %>%\n",
                "  summarise(tagged = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  pull(tagged)\n",
                "# inspect tagged text\n",
                "substr(ger_tagged[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pos-tag L1 Spanish data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "spa_tagged <- udpipe::udpipe_annotate(m_eng, spa) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::group_by(doc_id) %>%\n",
                "  summarise(tagged = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  pull(tagged)\n",
                "# inspect tagged text\n",
                "substr(spa_tagged[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pos-tag L1 Polish data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "pol_tagged <- udpipe::udpipe_annotate(m_eng, pol) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::group_by(doc_id) %>%\n",
                "  summarise(tagged = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  pull(tagged)\n",
                "# inspect tagged text\n",
                "substr(pol_tagged[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pos-tag L1 Russian data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "rus_tagged <- udpipe::udpipe_annotate(m_eng, rus) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::group_by(doc_id) %>%\n",
                "  summarise(tagged = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  pull(tagged)\n",
                "# inspect tagged text\n",
                "substr(rus_tagged[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can extract all the nouns from the four tagged subcorpora. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract nn\n",
                "ger_nns <- quanteda::kwic(tokens(ger_tagged, what = \"fastestword\"), \".*NN.*\", window = 10, valuetype = \"regex\", case_insensitive = F) %>% as.data.frame() %>% dplyr::select(-from, -to, -pattern) %>% dplyr::mutate(l1 = \"ger\")\n",
                "spa_nns <- quanteda::kwic(tokens(spa_tagged, what = \"fastestword\"), \".*NN.*\", window = 10, valuetype = \"regex\", case_insensitive = F) %>% as.data.frame() %>% dplyr::select(-from, -to, -pattern) %>% dplyr::mutate(l1 = \"spa\")\n",
                "pol_nns <- quanteda::kwic(tokens(pol_tagged, what = \"fastestword\"), \".*NN.*\", window = 10, valuetype = \"regex\", case_insensitive = F) %>% as.data.frame() %>% dplyr::select(-from, -to, -pattern) %>% dplyr::mutate(l1 = \"pol\")\n",
                "rus_nns <- quanteda::kwic(tokens(rus_tagged, what = \"fastestword\"), \".*NN.*\", window = 10, valuetype = \"regex\", case_insensitive = F) %>% as.data.frame() %>% dplyr::select(-from, -to, -pattern) %>% dplyr::mutate(l1 = \"rus\")\n",
                "# combine into one table\n",
                "nns <- rbind(ger_nns, spa_nns, pol_nns, rus_nns)\n",
                "# inspect data\n",
                "head(nns)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we check for each noun what type of noun it is and also if there is an article preceding it.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nns <- nns %>%\n",
                "  dplyr::mutate(tag = stringr::str_remove_all(keyword, \".*/\"),\n",
                "                noun = stringr::str_remove_all(keyword, \"/.*\"),\n",
                "                pre = stringr::str_remove_all(pre, \".*\\\\./\\\\.\"),\n",
                "                pre = stringr::str_remove_all(pre, \".*,/,\"),\n",
                "                pre = stringr::str_remove_all(pre, \".*VB[A-Z]{0,1}\")) %>%\n",
                "  dplyr::mutate(article = ifelse(stringr::str_detect(pre, \"DT|CD|PRP\"), 1, 0))\n",
                "# inspect\n",
                "head(nns)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tabulate results\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nnstb <- nns %>%\n",
                "  dplyr::group_by(tag, l1) %>%\n",
                "  dplyr::summarise(Freq = n(),\n",
                "                   Determiners = sum(article),\n",
                "                   Percent = round(Determiners/Freq*100, 1))\n",
                "# inspect results\n",
                "nnstb\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualize results\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nnstb %>%\n",
                "  dplyr::rename(Type = tag) %>%\n",
                "  dplyr::mutate(l1 = dplyr::case_when(l1 == \"ger\" ~ \"German\",\n",
                "                                      l1 == \"spa\" ~ \"Spanish\",\n",
                "                                      l1 == \"pol\" ~ \"Polish\",\n",
                "                                      l1 == \"rus\" ~ \"Russian\",\n",
                "                                      T ~ l1),\n",
                "                l1 = factor(l1, levels = c(\"German\", \"Spanish\", \"Polish\", \"Russian\"))) %>%\n",
                "  ggplot(aes(x = l1, y = Percent, fill = Type)) +\n",
                "  geom_bar(stat = \"identity\", position = position_dodge()) +\n",
                "  geom_text(aes(y = Percent+2, label = Percent), color = \"gray20\", size=3,position = position_dodge(0.9)) + \n",
                "  theme_bw() +\n",
                "  scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"gray80\"), \n",
                "                       name=\"Noun type\",\n",
                "                       breaks=c(\"NN\", \"NNS\", \"NNP\", \"NNPS\"),\n",
                "                       labels=c(\"Common noun singular\", \n",
                "                                \"Common noun plural\", \n",
                "                                \"Propoer noun singular\", \n",
                "                                \"Proper noun plural\")) +\n",
                "  labs(y = \"Percent of nouns preceeded by \\na determiner, cardial number, or possessive pronoun\", \n",
                "       x = \"Language background of learners\", \n",
                "       title = \"Percentage of noun types preceeded by determiners \\nacross language backgrounds of English learners\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We end the session by calling the session info which tells us what packages and what version of the software and packages we have used.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to HOME](https://github.com/MartinSchweinberger/SLAT7829Tutorials)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
