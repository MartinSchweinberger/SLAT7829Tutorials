{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![](https://github.com/MartinSchweinberger/SLAT7829/blob/master/images/bannerSLAT7829.jpeg?raw=true)\n",
                "\n",
                "# Case Study: Comparing British and American English\n",
                "\n",
                "This tutorial show how you can compare two corpora, the BROWN corpus which represents American English and the LOB corpus which represents British English.\n",
                "\n",
                "However, please keep in mind that the case studies  merely aim to exemplify ways in which R can be used in language-based research - rather than providing detailed procedures on how to do corpus-based research. \n",
                "\n",
                "\n",
                "\n",
                "## Preparation and session set up\n",
                "\n",
                "Activate required packages.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(dplyr)\n",
                "library(stringr)\n",
                "library(ggplot2)\n",
                "library(quanteda)\n",
                "library(quanteda.textstats)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading corpus data into R\n",
                "\n",
                "For the present tutorial, we will load the BROWN and the LOB corpus. Loading corpus data into R consists of two steps: \n",
                "\n",
                "1. create a list of paths of the corpus files\n",
                "\n",
                "2. loop over these paths and load the data in the files identified by the paths.\n",
                "\n",
                "To create a list of corpus files, you could use the code chunk below (the code chunk assumes that the BROWN data is in a folder called *BROWN* and the LOB data is in a folder called *LOB*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brownfiles <- list.files(here::here(\"BROWN\"), \n",
                "                          pattern = \".*.TXT\",\n",
                "                          full.names = T)\n",
                "lobfiles <- list.files(here::here(\"LOB\"), \n",
                "                          pattern = \".*.TXT\",\n",
                "                          full.names = T) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can then use the `sapply` function to loop over the paths and load the data int R using e.g. the `scan` function as shown below. In addition to loading the file content, we also paste all the content together using the `paste0` function and remove superfluous white spaces using the `str_squish` function from the `stringr` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brown <- sapply(brownfiles, function(x){\n",
                "  x <- scan(x, what = \"char\", sep = \"\", quote = \"\", \n",
                "            quiet = T, skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "lob <- sapply(lobfiles, function(x){\n",
                "  x <- scan(x, what = \"char\", sep = \"\", quote = \"\", \n",
                "            quiet = T, skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect data\n",
                "str(brown); str(lob)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have loaded your data into R, you can then continue with processing and transforming the data according to your needs.\n",
                "\n",
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "You can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "Then, click on the `New Folder` symbol and create a new folder and call it `MyData`.\n",
                "\n",
                "![Binder New Folder Symbol](https://slcladal.github.io/images/bindernewfolder.JPG)\n",
                "\n",
                "Then click on the upload symbol and upload your files into the `MyData` folder.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT**: here, we assume that you upload some form of text data - not tabular data!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mycorpus <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mycorpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "## Data processing\n",
                "\n",
                "In a first step, we clean the data. To do this, we inspect the first 200 characters of the data to see what it looks like and what we need to remove.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect first 200 characters of the brown corpus\n",
                "substr(brown[1], start=1, stop=200)\n",
                "# inspect first 200 characters of the lob corpus\n",
                "substr(lob[1], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The inspection shows that in the BROWN data, there are weird sequences like `A01 0010 1` or `A01 0010 1`. In the LOB corpus, there are also weird sequences like `A01 1` or `A01 2` and tags (pointy brackets and asterisks). As such, we remove these weird sequences from the corpus data. \n",
                "\n",
                "We start by cleaning the BROWN data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brown <- data.frame(rep(\"BROWN\", length(names(brown))), names(brown), brown) %>%\n",
                "  # change column names\n",
                "  dplyr::rename(corpus = 1, \n",
                "                file = 2,\n",
                "                content = 3) %>%\n",
                "  # shorten file names\n",
                "  dplyr::mutate(file = stringr::str_remove_all(file, \".*/\"),\n",
                "                file = stringr::str_remove_all(file, \".TXT\")) %>%\n",
                "  # clean corpus data\n",
                "  dplyr::mutate(content = stringr::str_remove_all(content, \"[A-Z][0-9]{2,2} [0-9]{4,4} [0-9]{1,5} \"),\n",
                "                content = stringr::str_remove_all(content, \"[^[:alnum:] ]\"),\n",
                "                content = stringr::str_remove_all(content, \" \\\\w \"))\n",
                "# inspect\n",
                "substr(brown[1, 3], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We also clean the LOB data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lob <- data.frame(rep(\"LOB\", length(names(lob))), names(lob), lob) %>%\n",
                "  # change column names\n",
                "  dplyr::rename(corpus = 1, \n",
                "                file = 2,\n",
                "                content = 3) %>%\n",
                "  # shorten file names\n",
                "  dplyr::mutate(file = stringr::str_remove_all(file, \".*/\"),\n",
                "                file = stringr::str_remove_all(file, \".TXT\")) %>%\n",
                "  # clean corpus data\n",
                "  dplyr::mutate(content = stringr::str_remove_all(content, \"[A-Z][0-9]{2,5} {0,5}[0-9]{1,5} \"),\n",
                "                content = stringr::str_remove_all(content, \"[^[:alnum:] ]\")) %>%\n",
                "  dplyr::mutate(content = stringr::str_replace_all(content, \" [0-9]{1,}(\\\\w{1,}) \", \" \\\\1 \"),\n",
                "                content = stringr::str_remove_all(content, \"[A-Z]{1,3}[0-9]{1,3}\"),\n",
                "                content = stringr::str_remove_all(content, \"[0-9]{1,3}[A-Z]{0,3}[a-z]{0,3}\"),\n",
                "                content = stringr::str_remove_all(content, \" \\\\w \"))\n",
                "# inspect\n",
                "substr(lob[1, 3], start=1, stop=200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now combine the two data frames into one.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus <- rbind(brown, lob)\n",
                "# inspect\n",
                "str(corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comparing the corpora \n",
                "\n",
                "There are many ways in which we can compare corpora. Here, we will  extract keywords that are characteristic for each of the two corpora and then visualize them in two different ways: in the form a bar graphs and as comparative word clouds.\n",
                "\n",
                "## Extracting Keywords \n",
                "\n",
                "When visualizing keywords in bar graphs, we first need to extract the keywords. this has the advantage that we can tabulate the keywords and inspect them before visualizing them.\n",
                "\n",
                "To extract the keywords, we combine the two corpora \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only select speeches by Obama and Trump\n",
                "comp_corpus <- quanteda::corpus(corpus$content, docvars = data.frame(file = corpus$file,\n",
                "                                                                  corpus = corpus$corpus))\n",
                "# inspect\n",
                "str(comp_corpus)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we convert the data into a document-frequency matrix (dfm) which shows how frequent words are in each document.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dfm grouped by president\n",
                "corp_dfm <- tokens(comp_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%\n",
                "  tokens_remove(stopwords(\"english\")) %>%\n",
                "  tokens_group(groups = corpus) %>%\n",
                "  dfm()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we calculate the keyness of words in the corpora.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculate keyness and determine Trump as target group\n",
                "result_keyness <- textstat_keyness(corp_dfm, \n",
                "                                   target = \"LOB\")\n",
                "# inspect\n",
                "head(result_keyness)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can then visualize the keywords as a bar graph using the `textplot_keyness` function that is provided in the `quanteda.textplots`  package.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot estimated word keyness\n",
                "quanteda.textplots::textplot_keyness(result_keyness) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparative Wordclouds \n",
                "\n",
                "\n",
                "We can use the `textplot_wordcloud` function to generate not only simple wordclouds but also comparative wordclouds that can be use to compare corpora.\n",
                "\n",
                "To generate a comparative wordcloud, you need to set the argument`comparison` to `TRUE` and we can specify  arguments to *prettify* the wordcloud. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# clean corpus\n",
                "corp_dfm %>%\n",
                "  # create word cloud\n",
                "  quanteda.textplots::textplot_wordcloud(comparison = TRUE, max_words = 100, rotation = .25)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We end the session by calling the session info which tells us what packages and what version of the software and packages we have used.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to HOME](https://github.com/MartinSchweinberger/SLAT7829Tutorials)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
